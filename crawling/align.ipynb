{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiyoon/miniconda3/envs/RecLLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-04 13:09:25.850733: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-04 13:09:25.870006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741093765.892143  909553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741093765.898733  909553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-04 13:09:25.922653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, XLNetModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Three men hammer on an anvil and pass a bottle...</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Lost 1892 French short animated film directed ...</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Lost short film consisting of 300 painted imag...</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>One night, Arlequin come to see his lover Colo...</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Performing on what looks like a small wooden s...</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>390752</td>\n",
       "      <td>tt0407808</td>\n",
       "      <td>Frog and Toad Are Friends</td>\n",
       "      <td>Claymation version of Arnold Lobel's story of ...</td>\n",
       "      <td>Animation,Comedy,Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>390753</td>\n",
       "      <td>tt0407810</td>\n",
       "      <td>From Ardoyne to the Áras: Inside the McAleese ...</td>\n",
       "      <td>Documentary on the private and public life of ...</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>390754</td>\n",
       "      <td>tt0407811</td>\n",
       "      <td>Frontstadt</td>\n",
       "      <td>A young filmmaker tries to gain a very persona...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>390755</td>\n",
       "      <td>tt0407815</td>\n",
       "      <td>Possible Changes</td>\n",
       "      <td>Two friends, Moon-ho and Jong-kyu, in their mi...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>390756</td>\n",
       "      <td>tt0407814</td>\n",
       "      <td>Full Grown Men</td>\n",
       "      <td>A man stuck in the reveries of his youth track...</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         id  \\\n",
       "0                0  tt0000005   \n",
       "1                1  tt0000004   \n",
       "2                2  tt0000002   \n",
       "3                3  tt0000003   \n",
       "4                4  tt0000001   \n",
       "...            ...        ...   \n",
       "207356      390752  tt0407808   \n",
       "207357      390753  tt0407810   \n",
       "207358      390754  tt0407811   \n",
       "207359      390755  tt0407815   \n",
       "207360      390756  tt0407814   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                        Blacksmith Scene   \n",
       "1                                             Un bon bock   \n",
       "2                                  Le clown et ses chiens   \n",
       "3                                            Poor Pierrot   \n",
       "4                                              Carmencita   \n",
       "...                                                   ...   \n",
       "207356                          Frog and Toad Are Friends   \n",
       "207357  From Ardoyne to the Áras: Inside the McAleese ...   \n",
       "207358                                         Frontstadt   \n",
       "207359                                   Possible Changes   \n",
       "207360                                     Full Grown Men   \n",
       "\n",
       "                                                     desc  \\\n",
       "0       Three men hammer on an anvil and pass a bottle...   \n",
       "1       Lost 1892 French short animated film directed ...   \n",
       "2       Lost short film consisting of 300 painted imag...   \n",
       "3       One night, Arlequin come to see his lover Colo...   \n",
       "4       Performing on what looks like a small wooden s...   \n",
       "...                                                   ...   \n",
       "207356  Claymation version of Arnold Lobel's story of ...   \n",
       "207357  Documentary on the private and public life of ...   \n",
       "207358  A young filmmaker tries to gain a very persona...   \n",
       "207359  Two friends, Moon-ho and Jong-kyu, in their mi...   \n",
       "207360  A man stuck in the reveries of his youth track...   \n",
       "\n",
       "                           genre  \n",
       "0                          Short  \n",
       "1                Animation,Short  \n",
       "2                Animation,Short  \n",
       "3       Animation,Comedy,Romance  \n",
       "4              Documentary,Short  \n",
       "...                          ...  \n",
       "207356   Animation,Comedy,Family  \n",
       "207357               Documentary  \n",
       "207358                     Drama  \n",
       "207359                     Drama  \n",
       "207360              Comedy,Drama  \n",
       "\n",
       "[207361 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('cleaned_imdb_genre.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primaryTitle과 description을 하나의 텍스트로 합치기\n",
    "df['text'] = df['title'].astype(str) + \" \" + df['desc'].astype(str)\n",
    "\n",
    "# genre 컬럼 전처리: 쉼표로 구분된 문자열을 리스트로 변환\n",
    "def process_genres(genres_str):\n",
    "    if pd.isna(genres_str):\n",
    "        return []\n",
    "    return [g.strip() for g in genres_str.split(',') if g.strip() != \"\"]\n",
    "\n",
    "df['genre_list'] = df['genre'].apply(process_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 장르: ['Action', 'Adult', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir', 'Game-Show', 'History', 'Horror', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi', 'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']\n"
     ]
    }
   ],
   "source": [
    "all_genres = set()\n",
    "for genres in df['genre_list']:\n",
    "    for genre in genres:\n",
    "        all_genres.add(genre)\n",
    "all_genres = sorted(list(all_genres))\n",
    "genre2id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "num_labels = len(all_genres)\n",
    "print(\"전체 장르:\", all_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 샘플에 대해 멀티핫 인코딩된 레이블 생성\n",
    "def encode_labels(genres):\n",
    "    label = [0] * num_labels\n",
    "    for g in genres:\n",
    "        if g in genre2id:\n",
    "            label[genre2id[g]] = 1\n",
    "    return label\n",
    "\n",
    "df['labels'] = df['genre_list'].apply(encode_labels)\n",
    "\n",
    "# 모델 학습에 필요한 열만 선택\n",
    "df_model = df[['text', 'genre_list', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blacksmith Scene Three men hammer on an anvil ...</td>\n",
       "      <td>[Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un bon bock Lost 1892 French short animated fi...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le clown et ses chiens Lost short film consist...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poor Pierrot One night, Arlequin come to see h...</td>\n",
       "      <td>[Animation, Comedy, Romance]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carmencita Performing on what looks like a sma...</td>\n",
       "      <td>[Documentary, Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>Frog and Toad Are Friends Claymation version o...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>From Ardoyne to the Áras: Inside the McAleese ...</td>\n",
       "      <td>[Documentary]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>Frontstadt A young filmmaker tries to gain a v...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>Possible Changes Two friends, Moon-ho and Jong...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>Full Grown Men A man stuck in the reveries of ...</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Blacksmith Scene Three men hammer on an anvil ...   \n",
       "1       Un bon bock Lost 1892 French short animated fi...   \n",
       "2       Le clown et ses chiens Lost short film consist...   \n",
       "3       Poor Pierrot One night, Arlequin come to see h...   \n",
       "4       Carmencita Performing on what looks like a sma...   \n",
       "...                                                   ...   \n",
       "207356  Frog and Toad Are Friends Claymation version o...   \n",
       "207357  From Ardoyne to the Áras: Inside the McAleese ...   \n",
       "207358  Frontstadt A young filmmaker tries to gain a v...   \n",
       "207359  Possible Changes Two friends, Moon-ho and Jong...   \n",
       "207360  Full Grown Men A man stuck in the reveries of ...   \n",
       "\n",
       "                          genre_list  \\\n",
       "0                            [Short]   \n",
       "1                 [Animation, Short]   \n",
       "2                 [Animation, Short]   \n",
       "3       [Animation, Comedy, Romance]   \n",
       "4               [Documentary, Short]   \n",
       "...                              ...   \n",
       "207356   [Animation, Comedy, Family]   \n",
       "207357                 [Documentary]   \n",
       "207358                       [Drama]   \n",
       "207359                       [Drama]   \n",
       "207360               [Comedy, Drama]   \n",
       "\n",
       "                                                   labels  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "207356  [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "207357  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "207358  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207359  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207360  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[207361 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_genre(df):\n",
    "    rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        genres = row['genre_list']\n",
    "        labels = row['labels']\n",
    "        \n",
    "        for genre in genres:\n",
    "            \n",
    "            # 새 데이터로 생성 (text, genre, genre_label)\n",
    "            rows.append({\n",
    "                'text': text,\n",
    "                'genre': genre,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 장르별로 텍스트와 라벨을 분리\n",
    "# genre_df = split_by_genre(df)\n",
    "\n",
    "# genre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_descriptions = {\n",
    "    \"Action\": \"This movie has thrilling action sequences with intense fight scenes.\",\n",
    "    \"Adult\": \"This film is intended for mature audiences, featuring explicit themes.\",\n",
    "    \"Adventure\": \"This movie takes the audience on an exciting journey full of discovery.\",\n",
    "    \"Animation\": \"This film is beautifully animated with vibrant characters and stunning visuals.\",\n",
    "    \"Biography\": \"This movie tells the true story of a remarkable person's life.\",\n",
    "    \"Comedy\": \"This movie is full of humor and laughter, guaranteed to entertain.\",\n",
    "    \"Crime\": \"This film revolves around criminal activities, investigations, and justice.\",\n",
    "    \"Documentary\": \"This is a factual film that explores real-life events and issues.\",\n",
    "    \"Drama\": \"This movie tells an emotional and heartfelt story with deep character development.\",\n",
    "    \"Family\": \"This movie is suitable for all ages, bringing warmth and joy to families.\",\n",
    "    \"Fantasy\": \"This film takes place in a magical world with fantastical elements and creatures.\",\n",
    "    \"Film-Noir\": \"This movie features a dark and mysterious atmosphere with complex characters.\",\n",
    "    \"Game-Show\": \"This show features competitive games and exciting challenges.\",\n",
    "    \"History\": \"This film brings historical events and figures to life with great detail.\",\n",
    "    \"Horror\": \"This movie contains scary and suspenseful moments that will keep you on edge.\",\n",
    "    \"Music\": \"This film revolves around music, featuring incredible performances and soundtracks.\",\n",
    "    \"Musical\": \"This movie is filled with songs and dance performances that tell a story.\",\n",
    "    \"Mystery\": \"This film keeps the audience guessing with twists and hidden secrets.\",\n",
    "    \"News\": \"This program covers current events and breaking news from around the world.\",\n",
    "    \"Reality-TV\": \"This show follows real people and their lives, providing entertainment and drama.\",\n",
    "    \"Romance\": \"A heartwarming romantic story unfolds in this film, full of love and emotions.\",\n",
    "    \"Sci-Fi\": \"This movie explores futuristic worlds, advanced technology, and space travel.\",\n",
    "    \"Short\": \"This is a short film that tells a compelling story in a brief runtime.\",\n",
    "    \"Sport\": \"This film is centered around sports, athletes, and competitive events.\",\n",
    "    \"Talk-Show\": \"This show features discussions, interviews, and engaging conversations.\",\n",
    "    \"Thriller\": \"This movie is filled with suspense, unexpected twists, and tension.\",\n",
    "    \"War\": \"This film portrays intense battles and the impact of war on people.\",\n",
    "    \"Western\": \"This movie is set in the Old West, featuring cowboys, duels, and frontier life.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = {\n",
    "    \"Action\": [\"Adventure\", \"Thriller\"],\n",
    "    \"Adult\": [\"Drama\", \"Romance\"],\n",
    "    \"Adventure\": [\"Fantasy\", \"Action\"],\n",
    "    \"Animation\": [\"Family\", \"Fantasy\"],\n",
    "    \"Biography\": [\"History\", \"Drama\"],\n",
    "    \"Comedy\": [\"Family\", \"Musical\"],\n",
    "    \"Crime\": [\"Thriller\", \"Drama\"],\n",
    "    \"Documentary\": [\"History\", \"News\"],\n",
    "    \"Drama\": [\"Romance\", \"Biography\"],\n",
    "    \"Family\": [\"Animation\", \"Comedy\"],\n",
    "    \"Fantasy\": [\"Sci-Fi\", \"Adventure\"],\n",
    "    \"Film-Noir\": [\"Mystery\", \"Thriller\"],\n",
    "    \"Game-Show\": [\"Reality-TV\", \"Talk-Show\"],\n",
    "    \"History\": [\"Biography\", \"Documentary\"],\n",
    "    \"Horror\": [\"Thriller\", \"Mystery\"],\n",
    "    \"Music\": [\"Musical\", \"Drama\"],\n",
    "    \"Musical\": [\"Music\", \"Comedy\"],\n",
    "    \"Mystery\": [\"Thriller\", \"Crime\"],\n",
    "    \"News\": [\"Documentary\", \"Talk-Show\"],\n",
    "    \"Reality-TV\": [\"Game-Show\", \"Talk-Show\"],\n",
    "    \"Romance\": [\"Drama\", \"Comedy\"],\n",
    "    \"Sci-Fi\": [\"Fantasy\", \"Action\"],\n",
    "    \"Short\": [\"Documentary\", \"Animation\"],\n",
    "    \"Sport\": [\"Drama\", \"Action\"],\n",
    "    \"Talk-Show\": [\"Reality-TV\", \"News\"],\n",
    "    \"Thriller\": [\"Horror\", \"Mystery\"],\n",
    "    \"War\": [\"History\", \"Drama\"],\n",
    "    \"Western\": [\"Adventure\", \"Action\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>genre_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Three men hammer on an anvil and pass a bottle...</td>\n",
       "      <td>[Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Lost 1892 French short animated film directed ...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Lost short film consisting of 300 painted imag...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>One night, Arlequin come to see his lover Colo...</td>\n",
       "      <td>[Animation, Comedy, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Performing on what looks like a small wooden s...</td>\n",
       "      <td>[Documentary, Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>Frog and Toad Are Friends</td>\n",
       "      <td>Claymation version of Arnold Lobel's story of ...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>From Ardoyne to the Áras: Inside the McAleese ...</td>\n",
       "      <td>Documentary on the private and public life of ...</td>\n",
       "      <td>[Documentary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>Frontstadt</td>\n",
       "      <td>A young filmmaker tries to gain a very persona...</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>Possible Changes</td>\n",
       "      <td>Two friends, Moon-ho and Jong-kyu, in their mi...</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>Full Grown Men</td>\n",
       "      <td>A man stuck in the reveries of his youth track...</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0                                        Blacksmith Scene   \n",
       "1                                             Un bon bock   \n",
       "2                                  Le clown et ses chiens   \n",
       "3                                            Poor Pierrot   \n",
       "4                                              Carmencita   \n",
       "...                                                   ...   \n",
       "207356                          Frog and Toad Are Friends   \n",
       "207357  From Ardoyne to the Áras: Inside the McAleese ...   \n",
       "207358                                         Frontstadt   \n",
       "207359                                   Possible Changes   \n",
       "207360                                     Full Grown Men   \n",
       "\n",
       "                                                     desc  \\\n",
       "0       Three men hammer on an anvil and pass a bottle...   \n",
       "1       Lost 1892 French short animated film directed ...   \n",
       "2       Lost short film consisting of 300 painted imag...   \n",
       "3       One night, Arlequin come to see his lover Colo...   \n",
       "4       Performing on what looks like a small wooden s...   \n",
       "...                                                   ...   \n",
       "207356  Claymation version of Arnold Lobel's story of ...   \n",
       "207357  Documentary on the private and public life of ...   \n",
       "207358  A young filmmaker tries to gain a very persona...   \n",
       "207359  Two friends, Moon-ho and Jong-kyu, in their mi...   \n",
       "207360  A man stuck in the reveries of his youth track...   \n",
       "\n",
       "                          genre_list  \n",
       "0                            [Short]  \n",
       "1                 [Animation, Short]  \n",
       "2                 [Animation, Short]  \n",
       "3       [Animation, Comedy, Romance]  \n",
       "4               [Documentary, Short]  \n",
       "...                              ...  \n",
       "207356   [Animation, Comedy, Family]  \n",
       "207357                 [Documentary]  \n",
       "207358                       [Drama]  \n",
       "207359                       [Drama]  \n",
       "207360               [Comedy, Drama]  \n",
       "\n",
       "[207361 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = df[['title', 'desc', 'genre_list']]\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Three men hammer on an anvil and pass a bottle...</td>\n",
       "      <td>Short</td>\n",
       "      <td>Blacksmith Scene Three men hammer on an anvil ...</td>\n",
       "      <td>[Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Lost 1892 French short animated film directed ...</td>\n",
       "      <td>Animation,Short</td>\n",
       "      <td>Un bon bock Lost 1892 French short animated fi...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Lost short film consisting of 300 painted imag...</td>\n",
       "      <td>Animation,Short</td>\n",
       "      <td>Le clown et ses chiens Lost short film consist...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>One night, Arlequin come to see his lover Colo...</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "      <td>Poor Pierrot One night, Arlequin come to see h...</td>\n",
       "      <td>[Animation, Comedy, Romance]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Performing on what looks like a small wooden s...</td>\n",
       "      <td>Documentary,Short</td>\n",
       "      <td>Carmencita Performing on what looks like a sma...</td>\n",
       "      <td>[Documentary, Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>390752</td>\n",
       "      <td>tt0407808</td>\n",
       "      <td>Frog and Toad Are Friends</td>\n",
       "      <td>Claymation version of Arnold Lobel's story of ...</td>\n",
       "      <td>Animation,Comedy,Family</td>\n",
       "      <td>Frog and Toad Are Friends Claymation version o...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>390753</td>\n",
       "      <td>tt0407810</td>\n",
       "      <td>From Ardoyne to the Áras: Inside the McAleese ...</td>\n",
       "      <td>Documentary on the private and public life of ...</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>From Ardoyne to the Áras: Inside the McAleese ...</td>\n",
       "      <td>[Documentary]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>390754</td>\n",
       "      <td>tt0407811</td>\n",
       "      <td>Frontstadt</td>\n",
       "      <td>A young filmmaker tries to gain a very persona...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Frontstadt A young filmmaker tries to gain a v...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>390755</td>\n",
       "      <td>tt0407815</td>\n",
       "      <td>Possible Changes</td>\n",
       "      <td>Two friends, Moon-ho and Jong-kyu, in their mi...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Possible Changes Two friends, Moon-ho and Jong...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>390756</td>\n",
       "      <td>tt0407814</td>\n",
       "      <td>Full Grown Men</td>\n",
       "      <td>A man stuck in the reveries of his youth track...</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "      <td>Full Grown Men A man stuck in the reveries of ...</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         id  \\\n",
       "0                0  tt0000005   \n",
       "1                1  tt0000004   \n",
       "2                2  tt0000002   \n",
       "3                3  tt0000003   \n",
       "4                4  tt0000001   \n",
       "...            ...        ...   \n",
       "207356      390752  tt0407808   \n",
       "207357      390753  tt0407810   \n",
       "207358      390754  tt0407811   \n",
       "207359      390755  tt0407815   \n",
       "207360      390756  tt0407814   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                        Blacksmith Scene   \n",
       "1                                             Un bon bock   \n",
       "2                                  Le clown et ses chiens   \n",
       "3                                            Poor Pierrot   \n",
       "4                                              Carmencita   \n",
       "...                                                   ...   \n",
       "207356                          Frog and Toad Are Friends   \n",
       "207357  From Ardoyne to the Áras: Inside the McAleese ...   \n",
       "207358                                         Frontstadt   \n",
       "207359                                   Possible Changes   \n",
       "207360                                     Full Grown Men   \n",
       "\n",
       "                                                     desc  \\\n",
       "0       Three men hammer on an anvil and pass a bottle...   \n",
       "1       Lost 1892 French short animated film directed ...   \n",
       "2       Lost short film consisting of 300 painted imag...   \n",
       "3       One night, Arlequin come to see his lover Colo...   \n",
       "4       Performing on what looks like a small wooden s...   \n",
       "...                                                   ...   \n",
       "207356  Claymation version of Arnold Lobel's story of ...   \n",
       "207357  Documentary on the private and public life of ...   \n",
       "207358  A young filmmaker tries to gain a very persona...   \n",
       "207359  Two friends, Moon-ho and Jong-kyu, in their mi...   \n",
       "207360  A man stuck in the reveries of his youth track...   \n",
       "\n",
       "                           genre  \\\n",
       "0                          Short   \n",
       "1                Animation,Short   \n",
       "2                Animation,Short   \n",
       "3       Animation,Comedy,Romance   \n",
       "4              Documentary,Short   \n",
       "...                          ...   \n",
       "207356   Animation,Comedy,Family   \n",
       "207357               Documentary   \n",
       "207358                     Drama   \n",
       "207359                     Drama   \n",
       "207360              Comedy,Drama   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Blacksmith Scene Three men hammer on an anvil ...   \n",
       "1       Un bon bock Lost 1892 French short animated fi...   \n",
       "2       Le clown et ses chiens Lost short film consist...   \n",
       "3       Poor Pierrot One night, Arlequin come to see h...   \n",
       "4       Carmencita Performing on what looks like a sma...   \n",
       "...                                                   ...   \n",
       "207356  Frog and Toad Are Friends Claymation version o...   \n",
       "207357  From Ardoyne to the Áras: Inside the McAleese ...   \n",
       "207358  Frontstadt A young filmmaker tries to gain a v...   \n",
       "207359  Possible Changes Two friends, Moon-ho and Jong...   \n",
       "207360  Full Grown Men A man stuck in the reveries of ...   \n",
       "\n",
       "                          genre_list  \\\n",
       "0                            [Short]   \n",
       "1                 [Animation, Short]   \n",
       "2                 [Animation, Short]   \n",
       "3       [Animation, Comedy, Romance]   \n",
       "4               [Documentary, Short]   \n",
       "...                              ...   \n",
       "207356   [Animation, Comedy, Family]   \n",
       "207357                 [Documentary]   \n",
       "207358                       [Drama]   \n",
       "207359                       [Drama]   \n",
       "207360               [Comedy, Drama]   \n",
       "\n",
       "                                                   labels  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "207356  [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "207357  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "207358  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207359  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207360  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[207361 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A4000. Max memory: 15.731 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 모델 로드 (8-bit 적용)\n",
    "MODEL_NAME = \"unsloth/phi-4-unsloth-bnb-4bit\"\n",
    "load_in_4bit = True\n",
    "max_seq_length = 1024\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 65,536,000 || all params: 14,725,043,200 || trainable%: 0.4451\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # self-attention 레이어\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # MLP 레이어 (장르 정보 인코딩 강화)\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()  # 학습 가능한 파라미터만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.to(model.dtype)  # 모델 dtype(BFloat16)과 맞추기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 65536000\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_filtered_negative_samples(genre_list, all_genres, num_neg_samples=3):\n",
    "    \n",
    "    negative_candidates = list(set(all_genres) - set(genre_list))\n",
    "\n",
    "    hard_neg_candidates = []\n",
    "    for genre in genre_list:\n",
    "        if genre in hard_negatives:\n",
    "            hard_neg_candidates.extend(hard_negatives[genre])\n",
    "\n",
    "    # Hard Negative 후보 중에서 실제 Negative 후보와 겹치는 것만 선택\n",
    "    hard_neg_candidates = list(set(hard_neg_candidates) & set(negative_candidates))\n",
    "\n",
    "    # 최종 Negative 샘플링 (Hard Negative + 추가 Negative)\n",
    "    if len(hard_neg_candidates) < num_neg_samples:\n",
    "        # Hard Negative가 부족하면 일반 Negative에서 추가\n",
    "        additional_negatives = list(set(negative_candidates) - set(hard_neg_candidates))\n",
    "        sampled_additional_negatives = random.sample(additional_negatives, num_neg_samples - len(hard_neg_candidates))\n",
    "        final_neg_samples = hard_neg_candidates + sampled_additional_negatives\n",
    "    else:\n",
    "        # Hard Negative가 충분하면 거기서만 샘플링\n",
    "        final_neg_samples = random.sample(hard_neg_candidates, num_neg_samples)\n",
    "\n",
    "    # 장르 설명 텍스트 변환\n",
    "    neg_texts = [label_descriptions[neg] for neg in final_neg_samples]\n",
    "\n",
    "    return neg_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, all_genres=None, max_length=128, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_genres = all_genres\n",
    "        self.max_length = max_length\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        title = row[\"title\"]\n",
    "        description = row[\"desc\"]\n",
    "        genre_list = row[\"genre_list\"]\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "\n",
    "            neg_texts = get_filtered_negative_samples(genre_list, all_genres)\n",
    "\n",
    "            story_prompt = f\"Movie Title: {title}, Story: {description}\"\n",
    "            label_prompt_pos = [\"Label: \" + label_descriptions[pos] for pos in genre_list]\n",
    "            label_prompt_neg = [\"Label: \" + neg for neg in neg_texts]  # 여러 개의 Negative 샘플\n",
    "\n",
    "            text_enc = self.tokenizer(story_prompt, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "            pos_enc = self.tokenizer(label_prompt_pos, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "            neg_enc = self.tokenizer(label_prompt_neg, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "            return {\n",
    "                \"text_input_ids\": text_enc[\"input_ids\"].squeeze(0),\n",
    "                \"text_attention_mask\": text_enc[\"attention_mask\"].squeeze(0),\n",
    "                \"positive_input_ids\": pos_enc[\"input_ids\"],\n",
    "                \"positive_attention_mask\": pos_enc[\"attention_mask\"],\n",
    "                \"negative_input_ids\": neg_enc[\"input_ids\"],\n",
    "                \"negative_attention_mask\": neg_enc[\"attention_mask\"]\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"answer\": genre_list\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(model_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ContrastiveDataset(train_df, tokenizer, all_genres=all_genres)\n",
    "val_dataset = ContrastiveDataset(val_df, tokenizer, all_genres=all_genres)\n",
    "test_dataset = ContrastiveDataset(val_df, tokenizer, mode=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    \"\"\"배치 내 `positive_input_ids`의 크기를 맞추는 함수\"\"\"\n",
    "\n",
    "    text_input_ids = torch.stack([b[\"text_input_ids\"] for b in batch])\n",
    "    text_attention_mask = torch.stack([b[\"text_attention_mask\"] for b in batch])\n",
    "\n",
    "    # 🔹 Positive 샘플 패딩 적용 (가장 큰 `num_positives` 기준)\n",
    "    max_pos_samples = max([b[\"positive_input_ids\"].shape[0] for b in batch])  # 배치 내 가장 긴 긍정 샘플 개수 찾기\n",
    "    pos_input_ids = [torch.cat([b[\"positive_input_ids\"], torch.zeros(max_pos_samples - b[\"positive_input_ids\"].shape[0], b[\"positive_input_ids\"].shape[1])]) if b[\"positive_input_ids\"].shape[0] < max_pos_samples else b[\"positive_input_ids\"] for b in batch]\n",
    "    pos_attention_mask = [torch.cat([b[\"positive_attention_mask\"], torch.zeros(max_pos_samples - b[\"positive_attention_mask\"].shape[0], b[\"positive_attention_mask\"].shape[1])]) if b[\"positive_attention_mask\"].shape[0] < max_pos_samples else b[\"positive_attention_mask\"] for b in batch]\n",
    "\n",
    "    pos_input_ids = torch.stack(pos_input_ids)\n",
    "    pos_attention_mask = torch.stack(pos_attention_mask)\n",
    "\n",
    "    # 🔹 Negative 샘플 (3개로 고정)\n",
    "    neg_input_ids = torch.stack([b[\"negative_input_ids\"] for b in batch])\n",
    "    neg_attention_mask = torch.stack([b[\"negative_attention_mask\"] for b in batch])\n",
    "\n",
    "    return {\n",
    "        \"text_input_ids\": text_input_ids,\n",
    "        \"text_attention_mask\": text_attention_mask,\n",
    "        \"positive_input_ids\": pos_input_ids.to(torch.long),\n",
    "        \"positive_attention_mask\": pos_attention_mask,\n",
    "        \"negative_input_ids\": neg_input_ids.to(torch.long),\n",
    "        \"negative_attention_mask\": neg_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Test 데이터에서 배치 크기 불일치 문제 해결\n",
    "    - `title`, `description`, `answer`를 리스트로 유지하여 DataLoader가 처리 가능하도록 함\n",
    "    \"\"\"\n",
    "    titles = [item[\"title\"] for item in batch]\n",
    "    descriptions = [item[\"description\"] for item in batch]\n",
    "    answers = [item[\"answer\"] for item in batch]  # 리스트 형태 유지\n",
    "\n",
    "    return {\n",
    "        \"title\": titles,\n",
    "        \"description\": descriptions,\n",
    "        \"answer\": answers\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=train_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=train_collate_fn)  # 검증 데이터는 shuffle X\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = model.dtype\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfoNCE 손실 함수 정의\n",
    "def info_nce_loss(query, positives, negatives, temperature=0.07):\n",
    "    query = F.normalize(query, p=2, dim=-1)\n",
    "    positives = F.normalize(positives, p=2, dim=-1)\n",
    "    negatives = F.normalize(negatives, p=2, dim=-1)\n",
    "\n",
    "    # 긍정 샘플들들과의 유사도\n",
    "    pos_sim = torch.exp(torch.matmul(query.unsqueeze(1), positives.permute(0, 2, 1)).squeeze(1) / temperature)  # (batch_size, num_pos_samples)\n",
    "\n",
    "    # 부정 샘플들과의 유사도 (여러 개의 부정 샘플 포함)\n",
    "    neg_sim = torch.exp(torch.matmul(query.unsqueeze(1), negatives.permute(0, 2, 1)).squeeze(1) / temperature)  # (batch_size, num_neg_samples)\n",
    "\n",
    "    # InfoNCE 손실 계산 (모든 부정 샘플을 고려)\n",
    "    pos_sim_sum = torch.sum(pos_sim, dim=-1)  # (batch_size)\n",
    "    neg_sim_sum = torch.sum(neg_sim, dim=-1)  # (batch_size)\n",
    "\n",
    "    # 🚀 Loss 계산\n",
    "    loss = -torch.log(pos_sim_sum / (pos_sim_sum + neg_sim_sum + 1e-8))\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['params'] = [p.to(model.dtype) for p in param_group['params']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    for param in param_group['params']:\n",
    "        print(f\"Optimizer Param dtype: {param.dtype}, Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.dtype}, requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  87%|████████▋ | 36054/41472 [69:14:40<10:16:24,  6.83s/it, loss=0.977]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 🚀 검증(Validation) 함수 정의\n",
    "def validation_step(model, val_dataloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, avg_train_loss, desc=\"Validating\", leave=False):\n",
    "\n",
    "            query_emb = model(batch[\"text_input_ids\"], batch[\"text_attention_mask\"]).last_hidden_state[:, 0]\n",
    "\n",
    "            batch_size, num_positives, seq_len = batch[\"positive_input_ids\"].shape  \n",
    "            pos_input_ids = batch[\"positive_input_ids\"].reshape(batch_size * num_positives, seq_len)\n",
    "            pos_attention_mask = batch[\"positive_attention_mask\"].reshape(batch_size * num_positives, seq_len)\n",
    "\n",
    "            pos_output = model(input_ids=pos_input_ids, attention_mask=pos_attention_mask, output_hidden_states=True)\n",
    "            pos_emb = pos_output.hidden_states[-1][:, 0].reshape(batch_size, num_positives, -1)  # 원래 배치 형태로 복구\n",
    "\n",
    "            batch_size, num_negatives, seq_len = batch[\"negative_input_ids\"].shape  # 현재 Negative Samples 크기 확인\n",
    "            #`negative_input_ids`를 `batch_size * num_negatives`로 Reshape\n",
    "            neg_input_ids = batch[\"negative_input_ids\"].reshape(batch_size * num_negatives, seq_len)\n",
    "            neg_attention_mask = batch[\"negative_attention_mask\"].reshape(batch_size * num_negatives, seq_len)\n",
    "\n",
    "            # Negative Embedding 얻기\n",
    "            neg_output = model(input_ids=neg_input_ids, attention_mask=neg_attention_mask, output_hidden_states=True)\n",
    "            neg_emb = neg_output.hidden_states[-1][:, 0].reshape(batch_size, num_negatives, -1)  # 원래 배치 형태로 복구\n",
    "\n",
    "\n",
    "            # InfoNCE 손실 계산\n",
    "            loss = info_nce_loss(query_emb, pos_emb, neg_emb)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # 🔥 예측값 생성 (query와 pos의 코사인 유사도를 기반으로 예측)\n",
    "            # query_emb = F.normalize(query_emb, p=2, dim=-1)\n",
    "            # pos_emb = F.normalize(pos_emb, p=2, dim=-1)\n",
    "            # similarity = torch.matmul(query_emb, pos_emb.T).cpu().numpy()\n",
    "            # preds = (similarity > 0.5).astype(int)  # Threshold = 0.5\n",
    "            # labels = torch.ones_like(similarity)\n",
    "\n",
    "            # all_preds.extend(preds.flatten())\n",
    "            # all_labels.extend(labels.flatten())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    # accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    wandb.log({\"Train Loss\": avg_train_loss, \"Val Loss\": avg_val_loss})\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "# 학습 루프 (tqdm 적용 + Validation 추가)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in train_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch[\"text_input_ids\"] = batch[\"text_input_ids\"].to(device)\n",
    "        batch[\"text_attention_mask\"] = batch[\"text_attention_mask\"].to(device)\n",
    "        batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device)\n",
    "        batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "        batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "        batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "\n",
    "        query_emb = model(batch[\"text_input_ids\"], batch[\"text_attention_mask\"], output_hidden_states=True).hidden_states[-1][:, 0]\n",
    "\n",
    "        batch_size, num_positives, seq_len = batch[\"positive_input_ids\"].shape  \n",
    "        pos_input_ids = batch[\"positive_input_ids\"].view(batch_size * num_positives, seq_len)\n",
    "        pos_attention_mask = batch[\"positive_attention_mask\"].view(batch_size * num_positives, seq_len)\n",
    "\n",
    "        pos_output = model(input_ids=pos_input_ids, attention_mask=pos_attention_mask, output_hidden_states=True)\n",
    "        pos_emb = pos_output.hidden_states[-1][:, 0].view(batch_size, num_positives, -1)  # 원래 배치 형태로 복구\n",
    "\n",
    "\n",
    "        batch_size, num_negatives, seq_len = batch[\"negative_input_ids\"].shape  # 현재 Negative Samples 크기 확인\n",
    "        #`negative_input_ids`를 `batch_size * num_negatives`로 Reshape\n",
    "        neg_input_ids = batch[\"negative_input_ids\"].view(batch_size * num_negatives, seq_len)\n",
    "        neg_attention_mask = batch[\"negative_attention_mask\"].view(batch_size * num_negatives, seq_len)\n",
    "\n",
    "        # Negative Embedding 얻기\n",
    "        neg_output = model(input_ids=neg_input_ids, attention_mask=neg_attention_mask, output_hidden_states=True)\n",
    "        neg_emb = neg_output.hidden_states[-1][:, 0].view(batch_size, num_negatives, -1)  # 원래 배치 형태로 복구\n",
    "\n",
    "        # InfoNCE 손실 계산\n",
    "        loss = info_nce_loss(query_emb, pos_emb, neg_emb)\n",
    "        loss = loss.to(model.dtype)\n",
    "\n",
    "        # print(f\"Loss dtype: {loss.dtype}, Model dtype: {model.dtype}\")\n",
    "        # for param in model.parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f\"Grad dtype: {param.grad.dtype}, Model dtype: {model.dtype}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # 🚀 검증 실행\n",
    "    avg_val_loss = validation_step(model, val_dataloader, avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 🚀 저장할 디렉토리 설정\n",
    "save_directory = \"fine_tuned_phi4_lora\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# ✅ LoRA Adapter 저장\n",
    "model.save_pretrained(save_directory)  # LoRA Adapter 저장\n",
    "tokenizer.save_pretrained(save_directory)  # 토크나이저 저장\n",
    "\n",
    "print(f\"✅ Model and LoRA adapter saved at {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 🔹 Hugging Face Text Generation Pipeline 설정\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"unsloth/phi-4-unsloth-bnb-4bit\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.50s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_llm_generation(test_dataloader, all_genres, output_file=\"llm_predictions.txt\"):\n",
    "    \"\"\"\n",
    "    - `unsloth/phi-4-unsloth-bnb-4bit` 모델을 사용하여 장르를 예측하고 평가\n",
    "    - `.txt` 파일에 `[Prompt] [LLM Predictions] [Answer]` 형식으로 저장\n",
    "    - Precision, Recall, F1-score 계산 후 파일에 추가\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"LLM Movie Genre Prediction Results\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}  # 타이틀, 설명, 정답 포함\n",
    "\n",
    "        # 🚀 프롬프트 메시지 생성 (대화형 메시지 포맷 적용)\n",
    "        prompts = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": f\"You are an AI movie genre classifier. Your task is to assign the most appropriate genres to a movie. Follow these rules: 1. Choose ONLY from the given genres: {', '.join(all_genres)}. 2. Output ONLY the predicted genres as a comma-separated list. 3. Do NOT repeat or copy the full genre list. 4. Do NOT add explanations or extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Movie Title: {title}, Story: {desc}, Predicted Genres:\"}\n",
    "            ]\n",
    "            for title, desc in zip(batch[\"title\"], batch[\"description\"])\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # 🚀 모델 예측 수행 (배치 단위로 처리)\n",
    "        outputs = [pipeline(prompt, max_new_tokens=15)[0][\"generated_text\"][-1] for prompt in prompts]\n",
    "\n",
    "        # 🚀 LLM이 생성한 장르 필터링 (올바른 장르만 포함)\n",
    "        filtered_preds = []\n",
    "        for pred in outputs:\n",
    "            pred_text = pred['content']\n",
    "            pred_lst = [genre.strip() for genre in pred_text.split(',')]\n",
    "            # pred_genres = [genre for genre in all_genres if genre in pred_lst]  # 정해진 장르 목록에 포함된 것만 선택\n",
    "            filtered_preds.append(pred_lst)\n",
    "\n",
    "        # 🚀 실제 정답 가져오기\n",
    "        actual_labels = batch[\"answer\"]\n",
    "\n",
    "        # 🚀 파일에 기록 (지정된 형식 적용)\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            for prompt, pred, actual in zip(prompts, filtered_preds, actual_labels):\n",
    "                f.write(f\"[Prompt]\\n{prompt}\\n\\n\")\n",
    "                f.write(f\"[LLM Predictions]\\n{', '.join(pred)}\\n\\n\")\n",
    "                f.write(f\"[Answer]\\n{', '.join(actual)}\\n\")\n",
    "                f.write(\"-\" * 100 + \"\\n\\n\")\n",
    "\n",
    "        # 🚀 모델의 예측값을 리스트로 변환\n",
    "        pred_vectors = [[1 if genre in pred else 0 for genre in all_genres] for pred in filtered_preds]\n",
    "        label_vectors = [[1 if genre in actual else 0 for genre in all_genres] for actual in actual_labels]\n",
    "\n",
    "        all_preds.extend(pred_vectors)\n",
    "        all_labels.extend(label_vectors)\n",
    "\n",
    "    # 🚀 Precision, Recall, F1-score 계산\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    print(f\"LLM Generation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "    # 🚀 평가 결과 파일에 저장\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        f.write(f\"Final Evaluation Metrics:\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1-score: {f1:.4f}\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Generation - Precision: 0.4044, Recall: 0.6624, F1-score: 0.4574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4044343079616163, 0.6624283520022937, 0.4574190720193066)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm_generation(test_dataloader, all_genres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecLLM",
   "language": "python",
   "name": "recllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
