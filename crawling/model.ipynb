{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjammy9087\u001b[0m (\u001b[33mjammy9087-pusan-national-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jiyoon/dev/RecSys_and_LLM/crawling/wandb/run-20250209_070812-5i8387iv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jammy9087-pusan-national-university/imdb_genre_classification/runs/5i8387iv' target=\"_blank\">jolly-lake-1</a></strong> to <a href='https://wandb.ai/jammy9087-pusan-national-university/imdb_genre_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jammy9087-pusan-national-university/imdb_genre_classification' target=\"_blank\">https://wandb.ai/jammy9087-pusan-national-university/imdb_genre_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jammy9087-pusan-national-university/imdb_genre_classification/runs/5i8387iv' target=\"_blank\">https://wandb.ai/jammy9087-pusan-national-university/imdb_genre_classification/runs/5i8387iv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"imdb_genre_classification\", config={\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 3,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_length\": 512,\n",
    "    \"num_workers\": 4,\n",
    "})\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb_dataset.csv\")\n",
    "\n",
    "# primaryTitle과 description을 하나의 텍스트로 합치기\n",
    "df['text'] = df['primaryTitle'].astype(str) + \" \" + df['description'].astype(str)\n",
    "\n",
    "# genre 컬럼 전처리: 쉼표로 구분된 문자열을 리스트로 변환\n",
    "def process_genres(genres_str):\n",
    "    if pd.isna(genres_str):\n",
    "        return []\n",
    "    return [g.strip() for g in genres_str.split(',') if g.strip() != \"\"]\n",
    "\n",
    "df['genre_list'] = df['genre'].apply(process_genres)\n",
    "\n",
    "# 전체 genre 목록 생성\n",
    "all_genres = set()\n",
    "for genres in df['genre_list']:\n",
    "    for genre in genres:\n",
    "        all_genres.add(genre)\n",
    "all_genres = sorted(list(all_genres))\n",
    "genre2id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "num_labels = len(all_genres)\n",
    "print(\"전체 장르:\", all_genres)\n",
    "\n",
    "# 각 샘플에 대해 멀티핫 인코딩된 레이블 생성\n",
    "def encode_labels(genres):\n",
    "    label = [0] * num_labels\n",
    "    for g in genres:\n",
    "        if g in genre2id:\n",
    "            label[genre2id[g]] = 1\n",
    "    return label\n",
    "\n",
    "df['labels'] = df['genre_list'].apply(encode_labels)\n",
    "\n",
    "# 모델 학습에 필요한 열만 선택\n",
    "df_model = df[['text', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "max_length = 512\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['text']\n",
    "        # label은 멀티레이블 멀티핫 인코딩 (리스트 형태)\n",
    "        label = torch.tensor(row['labels'], dtype=torch.float)\n",
    "        # 토큰화 (출력은 dict로, input_ids, attention_mask 등이 포함)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # 토크나이저 결과의 차원 제거 (batch dimension 제거)\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        encoding['labels'] = label\n",
    "        return encoding\n",
    "\n",
    "# Dataset 객체 생성\n",
    "dataset = IMDBDataset(df_model, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# random_split은 내부적으로 torch.Generator()를 사용해 seed 지정 가능 (재현성 위해)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # 사용 가능한 GPU 메모리 및 학습 속도에 따라 조정\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# BERT 모델 로드; 문제 유형을 multi_label_classification으로 설정하면,\n",
    "# 내부적으로 Sigmoid 활성화와 BCEWithLogitsLoss가 사용됩니다.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1} 시작\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} 완료: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"lr\": optimizer.param_groups[0]['lr'],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "print(f\"\\nTest Loss: {avg_test_loss:.4f}\")\n",
    "wandb.log({\"test_loss\": avg_test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bert_imdb_finetuned\")\n",
    "tokenizer.save_pretrained(\"bert_imdb_finetuned\")\n",
    "wandb.save(\"bert_imdb_finetuned/*\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
